{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "zzC_QrLFBPEk",
        "Xvw9XHxYBf4b"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_dir = '/content/drive/MyDrive/Machine_Learning_Project/'\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "from pandas.tseries.offsets import MonthEnd, MonthBegin\n",
        "\n",
        "import datetime "
      ],
      "metadata": {
        "id": "IDrUCt7bhfmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "664aacee-7f82-42d6-db6d-4520c7bd0bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import io\n",
        "from numpy.random import default_rng\n",
        "from sklearn.utils import resample\n",
        "from dateutil.relativedelta import relativedelta"
      ],
      "metadata": {
        "id": "H1IzZPIeKKUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "5S0ysqoRAszF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (1) Memory"
      ],
      "metadata": {
        "id": "zzC_QrLFBPEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To reduce memory space of each dataframe"
      ],
      "metadata": {
        "id": "bjdggewaBWzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_memory_usage(df, skip_cols=['permno', 'Date']):\n",
        "\n",
        "  \"\"\"\n",
        "  This function is to reduce the memory usage for each dataframe by downcasting the data type\n",
        "\n",
        "  Reference: T.A\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  for col in df.columns:\n",
        "        if col in skip_cols:\n",
        "            continue\n",
        "        col_type = df[col].dtype\n",
        "\n",
        "        if col_type != 'object':\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(\n",
        "                        np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(\n",
        "                        np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(\n",
        "                        np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(\n",
        "                        np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)\n",
        "\n",
        "            elif str(col_type)[:3] == \"flo\":\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(\n",
        "                        np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(\n",
        "                        np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    pass\n",
        "        else:\n",
        "            pass\n",
        "            #df[col] = df[col].astype('category')\n",
        "  return df"
      ],
      "metadata": {
        "id": "-U776UQHBTti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (2) Macroeconomic variables"
      ],
      "metadata": {
        "id": "Xvw9XHxYBf4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read this into standard memory so it doesn't take up RAM in function. \n",
        "\n",
        "#Current Market Rates\n",
        "market_rates_history = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/Machine Learning/Machine_Learning_Project/PMMS_history.csv', usecols=[\"date\", \"pmms30\"])\n",
        "market_rates_history[\"date\"] = pd.to_datetime(market_rates_history['date'].astype(str),  format='%m/%d/%Y')\n",
        "market_rates_history.drop(market_rates_history.loc[market_rates_history[\"date\"] < datetime.datetime(1999, 1, 1)].index, inplace = True)\n",
        "market_rates_history = market_rates_history.resample('M', on='date').mean()\n",
        "market_rates_history[\"date\"] = market_rates_history.index.get_level_values(0)+ pd.tseries.offsets.MonthBegin(-1)\n",
        "market_rates_history.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#HPI\n",
        "hpi_history = pd.read_csv(r'/content/drive/My Drive/Colab Notebooks/Machine Learning/Machine_Learning_Project/fmhpi_master_file.csv', usecols=[\"Year\", \"Month\", \"GEO_Name\", \"Index_SA\"])\n",
        "hpi_history[\"date\"] = pd.to_datetime(hpi_history[\"Year\"].astype(str) + '/' + hpi_history[\"Month\"].astype(str) + \"/1\")\n",
        "hpi_history[\"date\"] = pd.to_datetime(hpi_history['date'].astype(str),  format='%Y/%m/%d')\n",
        "hpi_history.drop(columns=[\"Year\", \"Month\"], inplace=True)\n",
        "hpi_history.drop(hpi_history.loc[hpi_history[\"date\"] < datetime.datetime(1999, 1, 1)].index, inplace = True)\n",
        "hpi_history.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Unemployment\n",
        "unemployment_history = pd.read_excel(r'/content/drive/My Drive/Colab Notebooks/Machine Learning/Machine_Learning_Project/unemployment_bystates.xlsx', usecols=[\"Year\", \"Month\", \"GEO_Name\", \"unemployment_rate\"])\n",
        "unemployment_history = unemployment_history.astype({\"Year\":\"int\", \"Month\":\"int\"})\n",
        "unemployment_history[\"date\"] = pd.to_datetime(unemployment_history[\"Year\"].astype(str) + '/' + unemployment_history[\"Month\"].astype(str) + \"/1\")\n",
        "unemployment_history[\"date\"] = pd.to_datetime(unemployment_history['date'].astype(str),  format='%Y/%m/%d')\n",
        "unemployment_history.drop(columns=[\"Year\", \"Month\"], inplace=True)\n",
        "unemployment_history.drop(unemployment_history.loc[unemployment_history[\"date\"] < datetime.datetime(1999, 1, 1)].index, inplace = True)\n",
        "unemployment_history.dropna(axis=0, inplace=True)\n",
        "unemployment_history.reset_index(drop=True, inplace=True)\n",
        "\n",
        "#Merge all together\n",
        "macro_variables = pd.merge(left = hpi_history,\n",
        "                           right = unemployment_history,\n",
        "                           how = \"inner\", on = [\"date\", \"GEO_Name\"],\n",
        "                           validate = \"1:1\")\n",
        "macro_variables = pd.merge(left = macro_variables,\n",
        "                           right = market_rates_history,\n",
        "                           how = \"left\", on = \"date\",\n",
        "                           validate = \"m:1\")"
      ],
      "metadata": {
        "id": "uaRcvZyTBqp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#macro_variables"
      ],
      "metadata": {
        "id": "vgt-kvFVBsmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (3) Single Loan-Level Dataset"
      ],
      "metadata": {
        "id": "bi7tyNVlE3ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = '/content/drive/MyDrive/Colab Notebooks/Machine Learning/Machine_Learning_Project/'"
      ],
      "metadata": {
        "id": "mnaKC1YXHtrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract columns to be used in the whole dataset"
      ],
      "metadata": {
        "id": "KFlX7AoaP1XQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "origination_col_names = {0: 'fico_ori',\n",
        "                         1: 'first_pmt_date',\n",
        "                         7: 'occupancy_stat',\n",
        "                         9: 'DTI_ori',\n",
        "                         10: 'UPB_ori',\n",
        "                         11: 'LTV_ori',\n",
        "                         12: 'origin_interest',\n",
        "                         14: 'ppm_flag',\n",
        "                         16: 'state',\n",
        "                         19: 'loan_seq_num',\n",
        "                         20: 'purpose'}\n",
        "\n",
        "performance_col_names = {0:'loan_seq_num',\n",
        "                         1: 'month',\n",
        "                         2: 'current_UPB',\n",
        "                         4: 'loan_age',\n",
        "                         5: 'time_to_maturity',\n",
        "                         10: 'current_interest'}"
      ],
      "metadata": {
        "id": "GbUo9mWzE8-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1. Open files"
      ],
      "metadata": {
        "id": "TJav3aWEIL6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to check duplicate\n",
        "\n",
        "def check_duplicates(temp_file):\n",
        "    if temp_file.duplicated().max() == True:\n",
        "        temp_file = temp_file.drop_duplicates()\n",
        "        #print(\"Duplicate rows discarded.\")\n",
        "    return temp_file"
      ],
      "metadata": {
        "id": "YeyC4gsnEaa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to set index\n",
        "\n",
        "def set_index(dataframe, *fields):\n",
        "    temp_list = []\n",
        "    for field in fields:\n",
        "        temp_list.append(field)\n",
        "    dataframe.set_index(keys=temp_list, inplace = True, verify_integrity = True)\n",
        "    dataframe.sort_index(inplace = True)"
      ],
      "metadata": {
        "id": "a9MC_E4mEgCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_origination(file_name, dir = dir, col_numbers = [0, 1, 7, 9, 10, 11, 12, 16, 19, 20], col_names = origination_col_names):\n",
        "    \"\"\"\"\n",
        "    This function assumes that drive is already mounted. Change data directory to the correct folder.\n",
        "  \n",
        "    Arguments:\n",
        "    - file_name [string] the name of the file, should be in format 'historical_data_YYYYQ#.txt', can pass this using below functions for file names\n",
        "    Return:\n",
        "    - temp_file [DataFrame] the completed DataFrame after checking for duplicates\n",
        "    \"\"\"\n",
        "\n",
        "    temp_file = pd.read_csv(dir + file_name, \n",
        "                            sep=\"|\", header=None, usecols= col_numbers).rename(columns = origination_col_names)\n",
        "  \n",
        "    temp_file.dropna(inplace = True)\n",
        "    check_duplicates(temp_file)\n",
        "\n",
        "    return temp_file"
      ],
      "metadata": {
        "id": "yqwkfowqP9DO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_monthly_performance(file_name, dir = dir, col_numbers = [0, 1, 2, 4, 5, 10], col_names = performance_col_names):\n",
        "    \"\"\"\"\n",
        "    This function assumes that drive is already mounted. Change data directory to the correct folder.\n",
        "  \n",
        "    Arguments:\n",
        "    - file_name [string] the name of the file, should be in format 'historical_data_time_YYYYQ#.txt', can pass this using below functions for file names\n",
        "    Return:\n",
        "    - temp_file [DataFrame] the completed DataFrame after checking for duplicates\n",
        "    \"\"\"\n",
        "\n",
        "    temp_file = pd.read_csv(dir + file_name, \n",
        "                            sep=\"|\", header=None, usecols= col_numbers).rename(columns = col_names)\n",
        "  \n",
        "    temp_file.dropna(inplace=True)\n",
        "    check_duplicates(temp_file)\n",
        "\n",
        "    return temp_file"
      ],
      "metadata": {
        "id": "IgTjekgeQB_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def file_name_ori(year, quarter):\n",
        "    \"\"\"\n",
        "    Returns the file name for the Origination file in the correct format\n",
        "\n",
        "    year, quarter = The year and quarter correspondng to the data files to be opened. year in [1999, 2021]; quarter in [1, 4]\n",
        "    \"\"\"\n",
        "    return \"historical_data_\" + str(year) + \"Q\" + str(quarter) + \".txt\"\n",
        "  \n",
        "def file_name_mon_per(year, quarter):\n",
        "    \"\"\"\n",
        "    Returns the file name for the Monthly Performance file in the correct format\n",
        "\n",
        "    year, quarter = The year and quarter correspondng to the data files to be opened. year in [1999, 2021]; quarter in [1, 4]\n",
        "    \"\"\"\n",
        "    return \"historical_data_time_\" + str(year) + \"Q\" + str(quarter) + \".txt\"\n"
      ],
      "metadata": {
        "id": "p4Vi5RraExml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2. Merge columns and sample data"
      ],
      "metadata": {
        "id": "6xkjMSAzQCsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `create_quarterly` function below will\n",
        "- load all the data associated with the loans originated within the specified quarter and year\n",
        "- sample data by the loan number sequence (`loan_seq_num`)\n",
        "- calculate the needed variables (e.g., `rolling_incentive`) for chosen loans\n",
        "- merge together the features of loans, the macroeconomic variables, and the calculated variables\n"
      ],
      "metadata": {
        "id": "LDblE-8ZSP4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Sampling Method__\n",
        "\n",
        "Consider the structure of the dataset:\n",
        "- Each data file from Freddie Mac contains all the data of all the loans originated within the quarter of the year that the file is named after\n",
        "- Suppose that there are $N$ loans originated in quarter $Q$ of year $YYYY$. Each loan is charaterized by a unique loan sequence number. Each record (i.e, each row) in the dataset reports the data associated with each loan at point in time (i.e., each month). \n",
        "- The total number of records is: <br> $\\large number\\ of\\ rows = \\Sigma^{N}_{i = 1} T_i$\n",
        "<br> where $T_i$ is the number of monthly records of $loan_i$\n",
        "\n",
        "After calculating all the needed variables, we sample __100,000__ random loan at any time t for each quarterly file"
      ],
      "metadata": {
        "id": "-xTpsBQQWF-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_quarterly(year, quarter):\n",
        "    \"\"\"\n",
        "    Imports quarterly data from external source, merge together Origination, Monthly Performance, and Macro Variables\n",
        "    Randomly sample loans, drop all other data points, calculate needed variables.\n",
        "\n",
        "    Arguments:\n",
        "    - year, quarter = The year and quarter correspondng to the data files to be opened. year in [1999, 2021]; quarter in [1, 4]\n",
        "\n",
        "    Returns \n",
        "    - quarterly_all [DataFrame] with 100,000 randomly sampled data points \n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    #----(1) Load \"Monthly Performance\" data for the year and quarter specified -----\n",
        "    #Base DataFrame from Monthly Performance file\n",
        "    quarterly_all = open_monthly_performance(file_name_mon_per(year, quarter))\n",
        "    quarterly_all[\"month\"] = pd.to_datetime(quarterly_all['month'].astype(str), format='%Y%m') \n",
        "    quarterly_all[\"month\"] = quarterly_all[\"month\"] + pd.tseries.offsets.MonthBegin(-1)\n",
        "    reduce_memory_usage(quarterly_all)\n",
        "    \n",
        "    \n",
        "    #-----(2) Load \"Origination\" data to merge with the \"Base DataFrame\"----------\n",
        "    temp_ori = open_origination(file_name_ori(year, quarter))\n",
        "    reduce_memory_usage(temp_ori)\n",
        "    quarterly_all = pd.merge(left = quarterly_all,\n",
        "                       right = temp_ori,\n",
        "                       how = \"left\", on = \"loan_seq_num\",\n",
        "                       validate = \"m:1\")\n",
        "    \n",
        "    del temp_ori\n",
        "    gc.collect()\n",
        "  \n",
        "\n",
        "    #------(3) Add macro variables to dataset ---------------------------------\n",
        "    #use only this for now to increase computing speed and reduce memory usage\n",
        "    quarterly_all = pd.merge(left = quarterly_all,\n",
        "                       right = macro_variables,\n",
        "                       how = \"left\", \n",
        "                       left_on = [\"month\", \"state\"],\n",
        "                       right_on = [\"date\", \"GEO_Name\"],\n",
        "                       validate = \"m:1\")\n",
        "    \n",
        "    #Downcast data before calculations\n",
        "    reduce_memory_usage(quarterly_all)\n",
        "    \n",
        "    #----(4) Calculate Interest Incentive and Rolling Interest Incentive--------\n",
        "    quarterly_all[\"interest_incentive\"] = quarterly_all[\"current_interest\"] - quarterly_all[\"pmms30\"]\n",
        "    quarterly_all['rolling_incentive'] = quarterly_all.groupby('loan_seq_num')['interest_incentive'].transform(lambda x: x.rolling(24, 1).mean())\n",
        "  \n",
        "    quarterly_all[\"previous_UPB\"] = quarterly_all.groupby(\"loan_seq_num\")[\"current_UPB\"].shift(1)\n",
        "    \n",
        "    quarterly_all.dropna(inplace = True, axis = 0)\n",
        "    \n",
        "    quarterly_all = resample(quarterly_all, replace=False, n_samples=100000, random_state=21)\n",
        "\n",
        "    #Housekeeping for speed and less RAM usage\n",
        "    set_index(quarterly_all, \"loan_seq_num\", \"month\")\n",
        "\n",
        "    \n",
        "    #------(5) Calculate remaining variables -----------------------------------\n",
        "  \n",
        "    quarterly_all['SATO'] = quarterly_all['origin_interest'] - quarterly_all.groupby('loan_seq_num')['pmms30'].transform(lambda x: x.iloc[0])\n",
        "  \n",
        "    quarterly_all['HPI_change'] = quarterly_all.groupby('loan_seq_num')['Index_SA'].transform(lambda x: x.div(x.iloc[0]).subtract(1))\n",
        "  \n",
        "    quarterly_all['pre_crisis'] = 0\n",
        "    crisis = pd.to_datetime('2009-10-1')\n",
        "    quarterly_all.loc[quarterly_all['date'] < crisis, 'pre_crisis'] = 1\n",
        "\n",
        "    quarterly_all['sin_month'] = np.sin((quarterly_all['date'].dt.month/6) * np.pi)\n",
        "    quarterly_all['cos_month'] = np.cos((quarterly_all['date'].dt.month/6) * np.pi)\n",
        "    \n",
        "    quarterly_all[\"current_prepay\"] = quarterly_all[\"previous_UPB\"] - quarterly_all[\"current_UPB\"]\n",
        "    quarterly_all[\"origin_interest\"] = quarterly_all[\"origin_interest\"] / (100 * 12)\n",
        "    quarterly_all[\"scheduled_payment\"] = (((quarterly_all[\"origin_interest\"] * quarterly_all[\"origin_UPB\"]) / \n",
        "                               (1 - np.power(1 + quarterly_all[\"origin_interest\"], -quarterly_all[\"origin_term\"]))) - \n",
        "                               (quarterly_all[\"origin_interest\"] * quarterly_all[\"previous_UPB\"]))\n",
        "    \n",
        "    quarterly_all[\"Payment\"] = quarterly_all.apply(lambda row : response_var(row), axis=1)\n",
        "  \n",
        "    return quarterly_all"
      ],
      "metadata": {
        "id": "f5fvVpPiFG9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (4) Data Preparing"
      ],
      "metadata": {
        "id": "m5ghJD3jI3au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "direc_output = '/content/drive/MyDrive/Colab Notebooks/Machine Learning/Machine_Learning_Project/Cleaned Data/'"
      ],
      "metadata": {
        "id": "cdJ_U-6DIY8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sd2_bzMOI91y",
        "outputId": "e455de25-90aa-4587-c80e-e54e5b774bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "years = np.range(1999, 2022)\n",
        "\n",
        "for year in years:\n",
        "  for quarter in tqdm(range(1, 4)):\n",
        "    data = create_quarterly(year, quarter)\n",
        "    data.to_csv(direc_output + str(year) + \"Q\" + str(quarter) +'.csv') "
      ],
      "metadata": {
        "id": "kGMmU6aPY2Wh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}