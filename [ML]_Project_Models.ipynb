{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Mortgage Prepayment Using Machine Learning Models"
      ],
      "metadata": {
        "id": "g-zI0gMQOQWv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Students__\n",
        "- Oanh Tran\n",
        "- Tianbo Liu\n",
        "- Peter Stefanowicz"
      ],
      "metadata": {
        "id": "VEjDO_PhOclH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Introduction"
      ],
      "metadata": {
        "id": "H4M1xkfLOn13"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A mortgage loan is one of the common way to borrow a large amount of money to cover a significant expense. Mortgage loans normally constitute to a substantial lending amount in financial institutions and banks. Thus, managing the risks associated with mortgage loans is of great importance.\n",
        "\n",
        "The predominant risks facing mortgage lenders include default, delinquency, and prepayment. Unlike default, which tends to happen in severe economic conditions or happen with some extreme cases of subprime mortgage borrowers, prepayment is likely to happen even under bussiness as usual circumstances and with borrowers of any credit-rating level. Not only do prepayments lower the profitability of mortgage loans, but they also expose the lenders to other financial risks such as liquidity risk and interest rate risk. \n",
        "\n",
        "Given such potential occurences and consequences of prepayments, our project aims at utilizing machine learning techniques and large dataset on mortgage loans to predict the types prepayment events associated with each individual loan.\n",
        "\n",
        "To present our work in detail, we divide this report into 3 main parts of __(A) Methodology__, __(B) Models and results__, and __(C) Discussions__. In the first part of _Methodology_, we will explain the structure of the loan-level dataset, the method to prepare and preprocess data, and the architecture of models to be built. In the second part of _Models and results_, we demonstrate, in details, the proces of data wrangling and model constructing. We also present the results of each step along the process. Finally, the last part of the report, _Discussion_, gives our comments and evaluation with regards to the results and model performance"
      ],
      "metadata": {
        "id": "lgJtOJQcOq9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Methodology\n",
        " "
      ],
      "metadata": {
        "id": "rFqIFVPQOs04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Target Variable"
      ],
      "metadata": {
        "id": "4oaC4Yy3tT5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We consider the __classification problem__, in which we aim at predicting potential prepayment event associated with an individual loan. The classification is based on the difference between the Unpaid Balance of the current time and that of the previous period. In order to obtain the target variable, we firstly calculate:\n",
        "- __`current_prep_amount`__: the current prepayment amount associated to each loan ($i$) at each point in time ($t$) \n",
        "- __`scheduled_payment`__: the monthly scheduled payment, implied by the terms in the contracts.\n",
        "\n",
        "Once the calculation is done, we categorize our target variable into 4 types based on the range of __`current_prep_amount`__ and the time __t__, at which the observation is examined (whether the time is maturity or not). \n",
        "\n",
        "We create the target variable __`prep_status`__, a categorical variable receiving possible values of $\\{ 1, 2, 3, 4 \\}$ defined as follow:\n",
        "\n",
        "1. No prepayment if \n",
        "  \n",
        "  __`current_prep_amount` $=$ `scheduled_amount`__ for any $t$\n",
        "\n",
        "2. Partial prepayment if \n",
        "\n",
        "  __`current_prep_amount` $\\in \\ ($ `scheduled_payment`, $UPB_{i, t-1}]$__\n",
        "  \n",
        "3. Fully prepayment if \n",
        "  \n",
        "  - __`current_prep_amount` $= 0$__, and\n",
        "  \n",
        "  - __t < maturity__\n",
        "4. Delinquency if \n",
        "  - __`current_prep_amount` < `scheduled_payment`__, and \n",
        "  - __t $\\le$ maturity__\n",
        "\n",
        "where\n",
        "- __`current_prep_amount`$ = UPB_{i, t-1} - UPB_{i, t}$__\n",
        "- $UPB_{it}$ is the unpaid principal balance of $loan_{i}$ at current time\n",
        "- $UPB_{i,t-1}$ is the unpaid principal balance of $loan_{i}$ at the previous period\n",
        "- __`scheduled_payment` $ \\large = \\frac {r.UBP_{i,0}}{1 - (1+r)^{-n}} - r.UPB_{i, t-1}$__ with $r$ being the monthly mortgage rate and $n$ being the original term of the loan in month"
      ],
      "metadata": {
        "id": "71rndwa6OxGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate response\n",
        "def response_var(row):\n",
        "    \"\"\"\n",
        "    Used in conjunction with df.apply() to calculate response variable\n",
        "    \"\"\"\n",
        "    \n",
        "    #No Prepayment\n",
        "    if row[\"current_prepay\"] == row[\"scheduled_payment\"]:\n",
        "        return \"NP\"\n",
        "    \n",
        "    #Partial Prepayment\n",
        "    if (row[\"current_prepay\"] > row[\"scheduled_payment\"]) & (row[\"current_prepay\"] <= row[\"previous_UPB\"]):\n",
        "        return \"PP\"\n",
        "    \n",
        "    #Full Prepayment\n",
        "    if (row[\"current_prepay\"] == 0) & (row[\"time_to_maturity\"] != 0):\n",
        "        return \"FP\"\n",
        "    \n",
        "    #Delinquency\n",
        "    if row[\"current_prepay\"] < row[\"scheduled_payment\"]:\n",
        "        return \"D\""
      ],
      "metadata": {
        "id": "ppgQh-PWTd0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Data"
      ],
      "metadata": {
        "id": "_Whb16q5tXyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the __Single Family Loan-Level Dataset__ from __Freddie Mac__ and some other macroeconomic variables namely unemployment rate, house price index, and 30-year fixed-rate mortgage.\n",
        "\n",
        "From the original datasets of Freddie Mac, we obtain data regarding \n",
        "- `loan_seq_num`: Loan sequence number, unique for each loan\n",
        "- `loan_size_t`: The original principal balance of each mortgage at each point in time\n",
        "- `ppm_flag`: indicates whether the mortgage is a _prepayment penalty mortgage_. A prepayment penalty mortgage with respect to which the borrower is, or at any time has been, obligated to pay a penalty in the event of certain prepayments of principal\n",
        "- `current_UPB`: The current unpaid principal balance, which is the mortgage ending balance as reported by the servicer for the corresponding monthly reporting period \n",
        "- `UPB_ori`: Unpaid Principal Balance at origination\n",
        "- `time_to_maturity`: Time to maturity of each loan\n",
        "- `date`: \n",
        "- `first_pmt_date`:\n",
        "- `interest_rate_it`: The current interest rate applied on corresponding client\n",
        "- `loan_age`: The number of months passed since the origination of the mortgage\n",
        "- `fico_ori`: The credit-score of the borrower\n",
        "- `LTV_ori`: The Loan-to-Value of borrower i, defined by the principal divided by the purchase price of the mortgage property\n",
        "- `DTI_ori`: The debt-to-income ratio defined as the sum of the borrower's monthly debt payments divided by the gross monthly income\n",
        "- `purpose`: The purpose of the mortgage loan when it was first originated\n",
        "- `owner_i`: The type of owner of the mortgage (owner-occupied, investment property, second home)\n",
        "- `state`: The state in which the property is located\n",
        "\n",
        "The macroeconomic variables include:\n",
        "- `unemployment_t`: the annual unemployment rate by states\n",
        "- `pmms`: The 30-year fixed rate mortgage\n",
        "- `HPI_change`: the rate of house price index change, calculated as $\\frac{HPI_{t} - HPI_{t-1}}{HPI_{t-1}}$\n",
        "\n",
        "In addition to data obtained from the original dataset, we also create other predictors\n",
        "- `interest_incentive_it`: The difference between the current mortgage market rate and the existing client's interest rate\n",
        "- `rolling_incentive_it`: 24-month moving average of interest rate incentive\n",
        "- `sato_i`: Spread-at-Origination, defined as the difference between the mortgage market rate and the original client's interest rate\n",
        "- `month_sin`: sine transformation of calendar months to capture the seasonality movement\n",
        "- `month_cos`: cosine transformation of calendar months to capture the seasonality movement\n",
        "- `precrisis_t`: indicating whether the observation falls in the period before or after crisis 2008"
      ],
      "metadata": {
        "id": "X3GAgZU4taQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Implementation"
      ],
      "metadata": {
        "id": "mw-pJ5bNuuZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a model using these datasets, we conduct the following steps:\n",
        "\n",
        "- __Step 1__: Data preparing. In this step, we load the data files of mortgage origination, monthly performance, macroeconomic variables, and create all the necessary predictors\n",
        "- __Step 2__: Data preprocessing. In this step, we normlize the data to bring the data back to similar scale\n",
        "- __Step 3__: Build the model\n"
      ],
      "metadata": {
        "id": "xXd-nLHTuwhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import modules\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "import datetime\n",
        "from dateutil.relativedelta import relativedelta"
      ],
      "metadata": {
        "id": "TW1d6GKxmHMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "XOrPmzIpYiBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRntWnHSuZN5",
        "outputId": "5ac8eef7-8016-4848-dd67-42e6010a412d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Directory to the datasets\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/Machine Learning/Machine_Learning_Project/Data_From_Peter/'"
      ],
      "metadata": {
        "id": "meWg1NpLsbAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----"
      ],
      "metadata": {
        "id": "hM3hSi9BDVPZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Model and Results"
      ],
      "metadata": {
        "id": "cFqbLa2NOxo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### I. Data Loading"
      ],
      "metadata": {
        "id": "EbnX3MJmvn6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Machine_Learning_Project/dataset.csv')"
      ],
      "metadata": {
        "id": "YVQ00fUqWRCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set index\n",
        "\n",
        "def set_index(dataframe, *fields):\n",
        "    temp_list = []\n",
        "    for field in fields:\n",
        "        temp_list.append(field)\n",
        "    dataframe.set_index(keys=temp_list, inplace = True, verify_integrity = True)\n",
        "    dataframe.sort_index(inplace = True)"
      ],
      "metadata": {
        "id": "5-DLuVNmvvYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_index(dataset, 'month', 'loan_seq_num')"
      ],
      "metadata": {
        "id": "s7zs18-Qv3eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"Payment\"] = dataset.apply(lambda row : response_var(row), axis=1)"
      ],
      "metadata": {
        "id": "sMGorAaJSE9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.to_csv('/content/drive/MyDrive/Colab Notebooks/Machine Learning/Machine_Learning_Project/dataset_final.csv')"
      ],
      "metadata": {
        "id": "12Aw1yRwaAfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "mInkSQG9fonF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### II. Split Dataset"
      ],
      "metadata": {
        "id": "xKQ3p_wvv6tj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split the dataset into training-validating-testing set by picking dates that give close enough percentage of the whole dataset"
      ],
      "metadata": {
        "id": "rR7uThAOv93m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#---Choose appropriate val_start date based on percentage of total dataset\n",
        "val_start = \"2018-01-01\"\n",
        "\n",
        "#---Choose appropriate test_start date based on percentage of total dataset\n",
        "test_start = \"2021-01-01\"\n",
        "\n",
        "#---Split the dataset\n",
        "train_val = dataset[dataset.index.get_level_values(0) < test_start]\n",
        "train = train_val[train_val.index.get_level_values(0) < val_start]\n",
        "val = train_val[train_val.index.get_level_values(0) >= val_start]\n",
        "test = dataset[dataset.index.get_level_values(0) >= test_start]\n",
        "\n",
        "print(\"Percentage of dataset in the validation set, given val_start date:\\n\" + \"\\u2500\"*66)\n",
        "print(\"{:^61}\".format(\"{:.4f}\".format(train_val[train_val.index.get_level_values(0) >= val_start].shape[0]*100 / train_val.shape[0]) + \" %\"))\n",
        "print(\"Percentage of dataset in the test set, given test_start date:\\n\" + \"\\u2500\"*61)\n",
        "print(\"{:^61}\".format(\"{:.4f}\".format(dataset[dataset.index.get_level_values(0) >= test_start].shape[0]*100 / dataset.shape[0]) + \" %\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrVPjlIBv_JY",
        "outputId": "2a20802a-22f3-41d9-865f-5ff671c9bc8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percentage of dataset in the validation set, given val_start date:\n",
            "──────────────────────────────────────────────────────────────────\n",
            "                          24.7872 %                          \n",
            "Percentage of dataset in the test set, given test_start date:\n",
            "─────────────────────────────────────────────────────────────\n",
            "                          13.6802 %                          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### III. Data Normalization"
      ],
      "metadata": {
        "id": "8wEzbSDMfjyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_data(dataset):\n",
        "  \"\"\"\n",
        "  This function is to normalize the dataset\n",
        "  Note: the function modifies directly the input dataset\n",
        "  \"\"\"\n",
        "\n",
        "  num_variables = []\n",
        "  cat_variables = []\n",
        "  for col in dataset.columns:\n",
        "    if ((dataset[col].dtype == 'int64') or (dataset[col].dtype == 'float64')):\n",
        "      num_variables.append(col)\n",
        "    else:\n",
        "      cat_variables.append(col)\n",
        "  \n",
        "\n",
        "  num_normalized = normalize(dataset.drop(columns = cat_variables),\n",
        "                             axis = 0)\n",
        "\n",
        "  for i in range(len(num_variables)):\n",
        "    dataset.loc[:, num_variables[i]] = num_normalized[:, i]\n",
        "\n",
        "  dataset[\"ppm_flag\"] = np.where(dataset[\"ppm_flag\"] == \"Y\", 1, 0)\n",
        "\n",
        "  dataset[\"occupancy_I\"] = np.where(dataset[\"occupancy_stat\"] == \"I\", 1, 0)\n",
        "  dataset[\"occupancy_P\"] = np.where(dataset[\"occupancy_stat\"] == \"P\", 1, 0)\n",
        "  dataset[\"occupancy_S\"] = np.where(dataset[\"occupancy_stat\"] == \"S\", 1, 0)\n",
        "\n",
        "  dataset[\"purpose_9\"] = np.where(dataset[\"purpose\"] == \"9\", 1, 0)\n",
        "  dataset[\"purpose_C\"] = np.where(dataset[\"purpose\"] == \"C\", 1, 0)\n",
        "  dataset[\"purpose_N\"] = np.where(dataset[\"purpose\"] == \"N\", 1, 0)\n",
        "  dataset[\"purpose_P\"] = np.where(dataset[\"purpose\"] == \"P\", 1, 0)\n",
        "\n",
        "  dataset[\"Payment_all\"] = np.zeros(dataset.shape[0])\n",
        "  dataset[\"Payment_all\"] = dataset[\"Payment_all\"].mask(dataset[\"Payment\"] == \"NP\", other = 0)\n",
        "  dataset[\"Payment_all\"] = dataset[\"Payment_all\"].mask(dataset[\"Payment\"] == \"PP\", other = 1)\n",
        "  dataset[\"Payment_all\"] = dataset[\"Payment_all\"].mask(dataset[\"Payment\"] == \"FP\", other = 2)\n",
        "  dataset[\"Payment_all\"] = dataset[\"Payment_all\"].mask(dataset[\"Payment\"] == \"D\", other = 3)\n",
        "\n",
        "  dataset.drop(columns = [\"date\", \"GEO_Name\", \"state\", \"occupancy_stat\", \"purpose\", \"Payment\"], inplace = True)"
      ],
      "metadata": {
        "id": "hnskdAf5rIHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (1) Normalize train set\n",
        "normalize_data(train)\n",
        "\n",
        "# (2) Normalize the validation set\n",
        "normalize_data(val)\n",
        "\n",
        "# (3) Normalize the test set\n",
        "normalize_data(test)"
      ],
      "metadata": {
        "id": "fLtk-Ik-rQGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x, train_y = train.drop(columns = [\"Payment_all\"]), train[\"Payment_all\"]\n",
        "val_x, val_y = val.drop(columns = [\"Payment_all\"]), val[\"Payment_all\"]\n",
        "test_x, test_y = test.drop(columns = [\"Payment_all\"]), test[\"Payment_all\"]"
      ],
      "metadata": {
        "id": "z_op6ODB1A69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "VZVplrGAu_-s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### IV. XGBoost"
      ],
      "metadata": {
        "id": "GC1CXe4hU5Jt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb"
      ],
      "metadata": {
        "id": "QmTHyXo1Yv2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics"
      ],
      "metadata": {
        "id": "8NmMUMV2iLA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tuning hyperparameters for XGBoost\n",
        "for eta in [.1, .01, .001]:\n",
        "    for max_depth in np.arange(4, 7):\n",
        "        #Create and train model; Use validation set and early stopping\n",
        "        xgb_model = xgb.XGBClassifier(n_estimators = 1000, \n",
        "                                      max_depth = max_depth,\n",
        "                                      learning_rate = eta,\n",
        "                                      num_class = 4,\n",
        "                                      tree_method = \"gpu_hist\",\n",
        "                                      eval_metric = \"merror\",\n",
        "                                      early_stopping_rounds=10,\n",
        "                                      n_jobs = 4,\n",
        "                                      objective = \"multi:softmax\",\n",
        "                                      random_state = 21)\n",
        "        xgb_model.fit(train_x, train_y,  eval_set = [(train_x, train_y), (val_x, val_y)], verbose = False)\n",
        "        \n",
        "        #Calculate Accuracy for Validation set to tune hyperparameters\n",
        "        accuracy_val = metrics.accuracy_score(y_true = val[\"Payment_all\"], \n",
        "                                              y_pred = xgb_model.predict(val.drop(columns = \"Payment_all\")),\n",
        "                                              normalize = True)\n",
        "        \n",
        "        #Print information for tuning of hyperparameters\n",
        "        print(\"Learning Rate: {:<5}        Max Depth: {}\\n\".format(eta, 4) + \"\\u2500\"*40)\n",
        "        print(\"{:^40}\".format('Validation Accuracy: {:.6f}'.format(accuracy_val)))\n",
        "        print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA0hHUjtU41g",
        "outputId": "61608065-5e83-432c-a25e-79530e7f3f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.1          Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.718549      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.1          Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.712051      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.1          Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.707179      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.01         Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.698708      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.01         Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.703028      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.01         Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.703053      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.001        Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.589143      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.001        Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.588756      \n",
            "\n",
            "\n",
            "\n",
            "Learning Rate: 0.001        Max Depth: 4\n",
            "────────────────────────────────────────\n",
            "     Validation Accuracy: 0.642555      \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above result, the model with the highest accuracy is the one with learning rate $\\eta = 0.1$ and `max_depth` = 4"
      ],
      "metadata": {
        "id": "JlMRHGkdwTJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Training optimal model\n",
        "\n",
        "optimal_eta = .1\n",
        "optimal_max_depth = 4\n",
        "xgb_model_final = xgb.XGBClassifier(n_estimators = 1000, \n",
        "                              max_depth = optimal_max_depth,\n",
        "                              learning_rate = optimal_eta,\n",
        "                              num_class = 4,\n",
        "                              tree_method = \"gpu_hist\",\n",
        "                              eval_metric = \"merror\",\n",
        "                              early_stopping_rounds=10,\n",
        "                              n_jobs = 4,\n",
        "                              objective = \"multi:softmax\",\n",
        "                              random_state = 21)\n",
        "xgb_model_final.fit(train_x, train_y,  eval_set = [(train_x, train_y), (val_x, val_y)], verbose = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tBK6UfcX1Hv_",
        "outputId": "faa72fac-c109-4d69-e310-bbdc5284e548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(early_stopping_rounds=10, eval_metric='merror', max_depth=4,\n",
              "              n_estimators=1000, n_jobs=4, num_class=4,\n",
              "              objective='multi:softprob', random_state=21,\n",
              "              tree_method='gpu_hist')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_XGB = xgb_model_final.predict(test_x)"
      ],
      "metadata": {
        "id": "aMTwHj5C1KvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_pred = y_pred_XGB, y_true = test_y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZlpJtCg1s6g",
        "outputId": "72944cca-dbcc-42c9-fa7a-e58568079be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.48      1.00      0.65    363601\n",
            "         2.0       1.00      0.87      0.93    205515\n",
            "         3.0       0.99      0.47      0.64    689459\n",
            "\n",
            "    accuracy                           0.69   1258575\n",
            "   macro avg       0.83      0.78      0.74   1258575\n",
            "weighted avg       0.85      0.69      0.69   1258575\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "HabKO2TsU3Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "pcx76V-3Q6lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "iH2a4AUyVK69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### V. Multi-Layer Perceptron Model"
      ],
      "metadata": {
        "id": "BKWt6Lm3VOAk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We attach a separate file for Deep Learning model under the name `multilayers`"
      ],
      "metadata": {
        "id": "c9qFJt-kVJlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VI. SVC"
      ],
      "metadata": {
        "id": "5JfCJUUc6T9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_val = pd.concat((train, val))\n",
        "\n",
        "X_train_val = train_val.drop(columns = ['Payment_all'])\n",
        "y_train_val = train_val['Payment_all']\n",
        "\n",
        "X_test = test.drop(columns = ['Payment_all'])\n",
        "y_test = test['Payment_all']\n",
        "\n",
        "svc = LinearSVC(penalty = 'l2', random_state = 2222)\n",
        "\n",
        "svc.fit(X = X_train_val,\n",
        "        y = y_train_val)\n",
        "\n",
        "y_pred_svc = svc.predict(X_test)"
      ],
      "metadata": {
        "id": "DUFKFRaU6UhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc.score(X = X_test,\n",
        "          y = y_test )"
      ],
      "metadata": {
        "id": "xA717d3FDCwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true = y_test,\n",
        "                      y_pred = y_pred_svc))"
      ],
      "metadata": {
        "id": "mreKqpXRDCJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion"
      ],
      "metadata": {
        "id": "WlqIqu5hO6h_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our test of three different kinds of models reveals not only the nature behind the risk of prepayment, but also the strengths and weaknesses The linear classification model, Support Vector Machines fared the worst of our three models. By comparison, the gradient-boosted decision tree model, XGBoost, and the deep learning model, Multi-Layered Perceptron, both did much better in predicting the outcomes of mortgage prepayment. This speaks to the nonlinear nature of this prediction. The latter two models both did somewhat similarly in terms of their predictive power. We believe this to be due to the fact that both excel in calculating the kind of dependencies we are dealing with in this dataset. This is also likely thanks in part to tuning of hyperparameters, allowing each to better fit the specific data we were tasked with training on and predicting.\n",
        "\n"
      ],
      "metadata": {
        "id": "sAtPGJYcO8co"
      }
    }
  ]
}